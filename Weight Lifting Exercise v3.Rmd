---
title: "Weight Lifting Exercise: A Qualitative Prediction"
output: html_document
---
####Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

#### Exploratory Data Analysis
Here, the data is loded and a summary is generated. There is a set of columns that have a lot of NA values. Remove those columns, and the other columns (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window) that would not be available outside a train/test environment and could be used to cheat in the prediction modeling. For example, all rows within the same window would have the same outcome and the prediction model would heavily weight the predicted outcome on the window. And the window would not be available in "live" data. 
```{r EDA1,echo=FALSE,warning=FALSE,error=FALSE,results='hide',message=FALSE}
library(caret)
library(RCurl)
set.seed(1135)
options(width = 200)
```
```{r EDA1b,echo=TRUE,warning=FALSE,error=FALSE,results='hide'}
wldata <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
               ssl.verifypeer=0L, followlocation=1L)
wtlift <- read.csv(text=wldata,na.strings=c("", "NA"));
```
```{r EDA2,echo=TRUE,warning=FALSE,error=FALSE,results='hide'}
# show summary of data
summary(wtlift);
# remove columns that have 19,000 or more NA values (window summary columns)
wtlift <- wtlift[,colSums(is.na(wtlift))<19000]
# remove columns related to window, subject, timestamps to avoid distracting model with artificial data
wtlift <- subset(wtlift, select=-c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))
```

#### Pull Training and Validation Samples
Pull 20% of data to use for training and 80% of data for out of sample error estimating.

```{r Partition,echo=TRUE,cache=TRUE}
inTrain = createDataPartition(y=wtlift$classe, p=0.2, list=FALSE)
wtlift_trn = wtlift[ inTrain,]
wtlift_val = wtlift[-inTrain,]

```
#### Cross-validation and Principal Control Analysis (PCA)
Perform cross validation with a K-fold strategy to produce 10 training and 10 test sets and repeat three times, then use average.
Parameters passed:  

* method = "repeatedcv"
* number = 10
* repeats = 3

Use PCA preprocessing to find number of variables required to capture 90% of the variation. These are included in the train calls.

* fitControl call: preProcOptions = list(thresh = 0.9)
* train call: preProcess="pca"

#### Stochastic Gradient Boosting
Thanks goes out to http://topepo.github.io/caret/training.html for guidance on use and description of Gradient Boosting Models.

For a gradient boosting machine (GBM) model, there are three main tuning parameters:

* number of iterations, i.e. trees, (called n.trees in the gbm function)
* complexity of the tree, called interaction.depth
* learning rate: how quickly the algorithm adapts, called shrinkage
* the minimum number of training set samples in a node to commence splitting (n.minobsinnode)

The default values tested for this model are shown in the first two columns. The column labeled "Accuracy" is the overall agreement rate averaged over cross-validation iterations. The agreement standard deviation is also calculated from the cross-validation results. The column "Kappa" is Cohen's (unweighted) Kappa statistic averaged across the resampling results. train works with specific models, and for these models, train can automatically create a grid of tuning parameters. By default, if p is the number of tuning parameters, the grid size is 3^p. As another example, regularized discriminant analysis (RDA) models have two parameters (gamma and lambda), both of which lie on [0, 1]. The default training grid would produce nine combinations in this two-dimensional space.

```{r Gradient Boosting,echo=TRUE,cache=TRUE,fig.width=7}
fitControl <- trainControl(
                           method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           preProcOptions = list(thresh = 0.9))

# Gradient Boosting Model
gbmFit1 <- train(classe ~ ., data = wtlift_trn,
                 method = "gbm",
                 preProcess="pca",
                 trControl = fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)

gbmFit1


```


#### Random Forest Model
Random Forest     rf
From the lecture notes, random forest is an extension of bagging on classification/regression trees and is one of the most used and accurate algorithms along with boosting.

process

   + bootstrap samples from training data (with replacement)
   + split and bootstrap variables
   + grow trees (repeat split/bootstrap) and vote/average final trees

drawbacks

   + algorithm can be slow (process large number of trees)
   + hard to interpret (large numbers of splits and nodes)
   + over-fitting (difficult to know which tree is causing over-fitting)
   + Note: it is extremely important to use cross validation when running random forest algorithms
   
The "mtry" parameter is the number of Randomly Selected Predictors   
```{r Random Forest,echo=TRUE}

rf_fitControl <- trainControl(
                           method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           preProcOptions = list(thresh = 0.9))

rfGrid = expand.grid(mtry = c(2,4,8,15))

rfFit1 <- train(classe ~ ., data = wtlift_trn,
                 method = "rf",
                 preProcess="pca",
                 trControl = rf_fitControl,
                 tuneGrid = rfGrid
)
rfFit1

trellis.par.set(caretTheme())
plot(rfFit1,ylim=c(0,1))
plot(rfFit1, metric = "Kappa",ylim=c(0,1))
```


#### Bagging
From the lecture notes, "bagging" is bootstrap aggregating with these attributes.

* resample training data set (with replacement) and recalculate predictions
* average the predictions together or majority vote
* averaging multiple complex models have similar bias as each of the models on its own, and reduced
variance because of the average
* most useful for non-linear models

B=10 in the train function represents the number of bootstrap samples to train over.

```{r Bagging,echo=TRUE,warning=FALSE,error=FALSE,message=FALSE}

bag_fitControl <- trainControl(
                           method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           preProcOptions = list(thresh = 0.9))

bagFit1 <- train(classe ~ .,
                 data = wtlift_trn,
                 method = "bagFDA",
                 # type = "prob",
                 B=10,
                 preProcess="pca",
                 trControl = bag_fitControl
)
bagFit1

```

#### Expected Out-of-Sample Error
The expected out-of-sample error is calculated using the validation data held out of the training data. We use the most accurate model, Random Forest, and predict on the validation data, then compare that prediction with the actuals using the "confusionMatrix" function. The out of sample error is 1-"Accuracy" in the output below.

```{r OOS Error,echo=TRUE}
# calculate outcome for validation data
val_pred <- predict(rfFit1,wtlift_val)
# compare results
confusionMatrix(wtlift_val$classe,val_pred)
```

#### Why Choices Were Made

* I chose the K-fold option of 10 folds based on comments in the discussion group suggesting that 3 was not enough, but 10 is good. 
* I chose the PCA of 90% because if reduced the run times on calculating the models.
* Bagging, Random Forrest, and Boosting models were chosend because they worked well with the model (type of data) and they have a reputation for accurate results.
* Random Forest produced the best results, so I used it for the out-of-sample error check